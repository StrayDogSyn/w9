#+TITLE: Introduction to Machine Learning Analysis and Modeling
#+AUTHOR: Evan Misshula
#+DATE: \today
#+LANGUAGE: en

#+LATEX_HEADER: \usepackage[style=apa, backend=biber]{biblatex}
#+LATEX_HEADER: \DeclareLanguageMapping{american}{american-apa}
#+LATEX_HEADER: \addbibresource{./refs/refs.bib}
#+LATEX_HEADER: \AtEveryBibitem{\clearfield{note}}
#+LATEX_HEADER: \usepackage{endnotes}
#+LATEX_HEADER: \let\footnote=\endnote
#+LATEX_HEADER: \usepackage{./jtc}
#+STARTUP: beamer
#+OPTIONS: H:2 toc:nil num:t
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [aspectratio=169]
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
g
#+name: initialize_lang
#+source: configuration
#+begin_src emacs-lisp :results output :exports none
  (require 'ob-mermaid)
  (setq ob-mermaid-cli-path "/home/evan/.nvm/versions/node/v20.1.0/bin/mmdc")
  ;; Doesn't work
	     ;; first it is necessary to ensure that Org-mode loads support for the
	      ;; languages used by code blocks in this article
	      (org-babel-do-load-languages
	       'org-babel-load-languages
	       '(
		 (ditaa      . t)     
		 (dot        . t)
		 (emacs-lisp . t)
		 (haskell    . t)
		 (org        . t)
		 (perl       . t)
		 (python     . t)
		 (R          . t)
		 (ruby       . t)
		 (plantuml   . t)
		 (mermaid    . t)
		 (sqlite     . t)))
	      ;; then we'll remove the need to confirm evaluation of each code
	      ;; block, NOTE: if you are concerned about execution of malicious code
	      ;; through code blocks, then comment out the following line
	  (add-to-list 'org-src-lang-modes '("plantuml" . plantuml))
	  (setq org-confirm-babel-evaluate nil)
	    (setq org-ditaa-jar-path "/usr/bin/ditaa")
	    (setq org-plantuml-jar-path "/usr/share/plantuml/plantuml.jar")
	    (add-to-list 'exec-path "/home/evan/.nvm/versions/node/v20.1.0/bin")
      ;;      (setq org-mermaid-jar-path "/home/evan/.nvm/versions/node/v20.1.0/lib/node_modules/@mermaid-js/mermaid-cli/node_modules/mermaid
      ;;    ")
    (setenv "PATH" (concat (getenv "PATH") ":/home/evan/.nvm/versions/node/v20.1.0/bin"))
    (add-to-list 'exec-path "/home/evan/.nvm/versions/node/v20.1.0/bin")

	   (setenv "PUPPETEER_EXECUTABLE_PATH" "/usr/bin/google-chrome-stable")
	   (setenv "PUPPETEER_DISABLE_SANDBOX" "1")
  (setq org-babel-mermaid-cli-path "/home/evan/.nvm/versions/node/v20.1.0/bin/mmdc")


	   (setenv "PATH" (concat "/home/evan/.nvm/versions/node/v20.1.0/bin:" (getenv "PATH")))
	    ;; finally we'll customize the default behavior of Org-mode code blocks
	      ;; so that they can be used to display examples of Org-mode syntax
	      (setf org-babel-default-header-args:org '((:exports . "code")))
	      (setq org-babel-inline-result-wrap '%s)
	      ;; This gets rid of the wrapping around the results of evaluated org mode 
	      ;; in line code
	      (setq reftex-default-bibliography '("/home/emisshula/proposal/mybib.bib"))
	      (setq org-latex-prefer-user-labels t)
      (plist-put org-format-latex-options :scale 3.0)
      (global-set-key (kbd "C-c e") 'insEq)
#+end_src

#+RESULTS: configuration

* Ingestion & Preprocessing
** End to end process                                          :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Recall *ML workflow* is a sequence of steps to build and deploy a model that
solves a problem using data.

** The pipeline                                                     :B_block::
:BEAMER_env: block
:END:

| Ingestion & Preprocessing | *Analysis*            | Modeling   | Deployment |
|---------------------------+-----------------------+------------+------------|
| Definition                | *EDA*                 | Selection  | Tuning     |
| Data Collection           | *Feature Engineering* | Training   | Deployment |
| Cleaning                  |                       | Evaluation | Monitoring |

** ML Workflow Graph                                                :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
#+CAPTION: ML workflow steps rendered as a flowchart
#+ATTR_LATEX: :width=0.8\linewidth
[[file:workflow.png]]


#+begin_src mermaid :file workflow.png  :exports results
  graph LR
    A[Ingestion] --> B[Analysis]
    B --> C[Modeling]
    C --> D[Deployment]
#+end_src

#+RESULTS:
[[file:workflow.png]]

* Univariate EDA
** Exploratory Data Analysis                                        :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

Exploratory Data Analysis (EDA) in the context of machine learning
(ML) refers to the systematic process of examining and visualizing the
structure, patterns, anomalies, and relationships within a dataset
before applying machine learning algorithms. The goal is to gain
intuition and insight about the data to inform: Understand the
distribution of each feature (e.g., normality, skewness, outliers).

Assess relationships between input features and the target variable
(e.g., correlation, mutual information).


** Exploratory Data Analysis in Pandas
:PROPERTIES:
:BEAMER_env: frame
:END:
Pandas tools:
*** `.info()` – Structure Overview
#+BEGIN_SRC python
df.info()
#+END_SRC

#+RESULTS:

- Displays a concise summary of the DataFrame:
  - Number of non-null values per column
  - Data types of each column
  - Memory usage
  - Total number of rows and columns

*** Pandas example                                                  :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
#+BEGIN_EXAMPLE
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000 entries, 0 to 999
Data columns (total 5 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   name      1000 non-null   object 
 1   age       950 non-null    float64
 2   income    1000 non-null   float64
 3   city      1000 non-null   object 
 4   joined    995 non-null    datetime64[ns]
dtypes: datetime64 , float64(2), object(2)
memory usage: 39.2+ KB
#+END_EXAMPLE
** Pandas `.describe()`                                             :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
*** `.describe()` – Summary Statistics
#+BEGIN_SRC python
df.describe()
#+END_SRC

#+RESULTS:

- Returns summary statistics for numeric columns:
  - `count`, `mean`, `std`, `min`, `25%`, `50%`, `75%`, `max`

#+BEGIN_EXAMPLE
              age      income
count   950.000000  1000.000000
mean     35.5       60000.0
std      10.0       15000.0
min      18.0       20000.0
25%      28.0       50000.0
50%      35.0       60000.0
75%      42.0       70000.0
max      65.0       120000.0
#+END_EXAMPLE
** Non-numeric results                                              :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
For non-numeric columns:

#+BEGIN_SRC python
df.describe(include='object')
#+END_SRC

#+RESULTS:

- Shows: `count`, `unique`, `top`, `freq`

** Pandas Tool Summary                                              :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
#+ATTR_BEAMER: :overlay +-
| Method         | Purpose                      | Applies To        |
|----------------|------------------------------|-------------------|
| `df.info()`     | Structure & metadata         | All columns       |
| `df.describe()` | Descriptive stats (summary)  | Numeric by default|

** Univariate analysis Visualize
*** *Look at your data*
- Histogram + KDE \(\rightarrow\) quick skew/kurtosis check.
- Q-Q Plot \(\rightarrow\) best for tail behavior.
- Boxplot \(\rightarrow\) highlights symmetry and outliers.
[Live Code 2]

** Univariate Analysis Tests
:PROPERTIES:
:BEAMER_env: frame
:END:

*** Tests for Skewness and Kurtosis
- *D'Agostino's \(K^2\) Test*: Combines measures of skewness and kurtosis.
  - Based on transformations of the sample skewness and kurtosis.
  - Null Hypothesis: The data is normally distributed.
  - Available in `scipy.stats.normaltest`. 

 - **Jarque–Bera Test**:
  - Specifically evaluates skewness and excess kurtosis against a normal distribution.
  - Null Hypothesis: Data is normally distributed.
   
** Summary Univariate Analysis                                      :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:
*** Interpretation
- *Low p-value (< 0.05)*: Reject null \(\rightarrow\) evidence of
  non-normal skew/kurtosis.
- *High p-value (≥ 0.05)*: Fail to reject null \(\rightarrow\) no
  evidence of non-normality.

* Multivariate EDA
** Motivation for Multivariate EDA 
:PROPERTIES:
:BEAMER_ENV: frame
:END:
- Univariate EDA is insufficient for understanding dependencies and
  structure in multivariate data.
- Multivariate EDA focuses on relationships, redundancy, and
  conditional structure across features.
- Goal: Identify informative, redundant, or interacting features.

** Joint and Marginal Distributions
:PROPERTIES:
:BEAMER_ENV: frame
:END:
- Let \( X = (X_1, X_2, \ldots, X_d) \in \mathbb{R}^d \) be a random
  vector.
- The *joint distribution* \( P_X \) describes full probabilistic
  structure.
- The *marginal distribution* of a feature \( X_i \) is obtained by
  integrating out all other variables.
- Understanding joint vs. marginal behavior is central to multivariate
  EDA.

** Statistical Dependence
:PROPERTIES:
:BEAMER_ENV: frame
:END:
- Two variables \( X \) and \( Y \) are independent if:
  \[
  P_{X,Y}(x, y) = P_X(x) P_Y(y)
  \]
- EDA seeks to *discover* dependencies between variables.
- Classical tools: covariance, correlation — but these are limited to
  linear dependence.

** Mutual Information
:PROPERTIES:
:BEAMER_ENV: frame
:END:
- Mutual Information (MI) is a nonparametric measure of dependence:
  \[
  I(X; Y) = \int \int p(x,y) \log \left( \frac{p(x,y)}{p(x)p(y)} \right) dx dy
  \]
- \( I(X;Y) = 0 \) iff \( X \perp Y \).
- Captures all kinds of dependence — not just linear.

** Connection to KL Divergence
:PROPERTIES:
:BEAMER_ENV: frame
:END:
- Mutual Information is a special case of the *Kullback-Leibler
  divergence*:
  \[
  I(X;Y) = D_{\mathrm{KL}}(P_{X,Y} \| P_X \otimes P_Y)
  \]
- It measures how far the joint distribution is from the product of the marginals.
- Interpreted as: *How surprising is the joint distribution, compared to independence?*

** Why It Matters in EDA
:PROPERTIES:
:BEAMER_ENV: frame
:END:
- Helps detect feature redundancy or relevance.
- Basis for feature selection and structure learning.
- Multivariate visualizations (pair plots, heatmaps, etc.) are
  motivated by mathematical notions of dependence.
[Live Code]
* KL Divergence
** What is KL Divergence?                                           :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

KL Divergence is a measure of how one probability distribution \( Q \)
differs from a reference distribution \( P \).

- It is not symmetric: \( D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P) \)
- KL divergence is always non-negative: \( D_{KL}(P \parallel Q) \geq 0 \)
- \( D_{KL}(P \parallel Q) = 0 \) if and only if \( P = Q \) almost everywhere

** Mathematical Definition (Discrete)                               :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

Let \( P \) and \( Q \) be probability mass functions over a finite or
countable set \( \mathcal{X} \).

\begin{equation}
D_{KL}(P \parallel Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}
\end{equation}

- The log is typically taken to base 2 (bits) or base \( e \) (nats)
- Requires \( Q(x) > 0 \) wherever \( P(x) > 0 \)

** Interpretation                                                   :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

- Measures the expected number of *extra bits* needed to code samples
  from \( P \) using a code optimized for \( Q \)
- It is the *relative entropy* of \( P \) with respect to \( Q \)

** Mathematical Definition (Continuous)                             :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

Let \( p(x) \) and \( q(x) \) be probability density functions over a
domain \( \mathcal{X} \subseteq \mathbb{R}^n \):

\begin{equation}
D_{KL}(P \parallel Q) = \int_{\mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \, dx
\end{equation}

- Again, the divergence is zero iff \( p(x) = q(x) \) almost everywhere

** Practical Calculation                                            :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

Given empirical data samples \( x_1, \dots, x_n \sim P \), estimate KL divergence:

- Use histograms or kernel density estimators (KDE) to estimate \( p(x) \), \( q(x) \)
- Approximate:

\begin{equation}
\hat{D}_{KL}(P \parallel Q) = \frac{1}{n} \sum_{i=1}^n \log \frac{p(x_i)}{q(x_i)}
\end{equation}

- Common in variational inference and mutual information estimation

** Summary of KL Divergence                                         :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

- KL divergence quantifies divergence from a reference distribution
- Central to many ML methods: variational inference, GANs, language modeling
- Not symmetric, not a true metric
- Requires careful estimation for continuous variables

* Feature Engineering

** What is Feature Engineering?                                     :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Feature engineering is the process of transforming raw data into
  meaningful input features for machine learning models.
- It involves:
  - Creating new features
  - Modifying existing ones
  - Selecting the most relevant subset
- The goal is to enhance model performance by exposing the most useful signal in the data.

** Why is Feature Engineering Important?                            :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Quality of features often outweighs choice of algorithm.
- Poor features = poor model performance, regardless of the model used.
- Good features can:
  - Improve accuracy
  - Speed up training
  - Reduce overfitting
  - Make models interpretable

** Common Types of Feature Engineering                              :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- *Normalization/Scaling*: StandardScaler, MinMaxScaler
- *Encoding*: One-hot, Label encoding
- *Discretization/Binning*
- *Polynomial Features*: Capture interactions
- *Date/Time decomposition*: Day, month, weekday, etc.
- *Log transformations*: For skewed distributions

** Feature Selection and Extraction                                 :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- *Feature Selection*: Identify and keep the most relevant variables.
  - *RFE (Recursive Feature Elimination)*:
    - Iteratively builds a model and removes the least important feature.
    - Works with any estimator that exposes `coef_` or `feature_importances_`.
- *Feature Extraction*: Derive new features from raw data.
  - *t-SNE (t-distributed Stochastic Neighbor Embedding)*:
    - A nonlinear dimensionality reduction technique.
    - Preserves local structure; useful for visualizing high-dimensional data.
  - *UMAP (Uniform Manifold Approximation and Projection)*:
    - Similar to t-SNE but faster and better preserves global structure.
    - Based on topological and geometric foundations.

** Best Practices and Guidelines                                    :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Understand the data context and business goals.
- Visualize feature distributions and relationships.
- Watch out for data leakage.
- Use cross-validation to evaluate engineered features.

** Summary of Feature Engineering                                   :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Feature engineering is essential for successful modeling.
- Methods like RFE, t-SNE, and UMAP help in selection and dimensionality reduction.
- Combining domain knowledge with statistics is key.

