% Created 2025-06-08 Sun 09:54
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{enumitem}
\setlist[description]{style=nextline}
\author{Evan Misshula}
\date{\today}
\title{Machine Learning Glossary}
\hypersetup{
 pdfauthor={Evan Misshula},
 pdftitle={Machine Learning Glossary},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.3 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section{Glossary}
\label{sec:orgd8867bf}

\subsection{Section 1: ML Concepts and Isolation Forests}
\label{sec:orgbd7edd6}
\begin{description}
\item[{Anomaly Score}] A value between 0 and 1 indicating how easily a data point can be isolated in an isolation forest; higher values suggest anomalies.
\item[{Classification}] A machine learning task that assigns labels to input data from a set of discrete categories.
\item[{Data Cleaning}] The process of correcting or removing inaccurate records from a dataset, including handling missing values, outliers, and duplicates.
\item[{Data Collection}] The act of gathering and measuring information from various sources to answer a research question or model a problem.
\item[{Dependent Variable}] The target or outcome variable that a model is trained to predict, also known as label, response, or outcome variable.
\item[{Duplicate}] A record that appears more than once in a dataset and represents the same real-world entity.
\item[{Expected Path Length}] The average number of steps required to isolate a data point across all trees in an isolation forest.
\item[{Feature Engineering}] The process of selecting, modifying, or creating new features from raw data to improve model performance.
\item[{Isolation Forest}] An anomaly detection method that isolates observations by randomly selecting features and split values.
\item[{Isolation Tree}] A binary tree used in isolation forests where each node splits data based on randomly chosen feature and split value.
\item[{Missing Value}] A data entry where no value is recorded, often due to errors in data collection or system limitations.
\item[{Model Deployment}] The process of integrating a trained machine learning model into a production environment where it can make predictions.
\item[{Outlier}] A data point that differs significantly from other observations and may indicate an anomaly or data error.
\item[{Path Length}] The number of edges from the root of an isolation tree to the node where a data point is isolated.
\item[{Regression}] A type of machine learning task that involves predicting continuous numeric values.
\item[{Tree Depth}] The length of the longest path from the root to a leaf in a tree structure.
\item[{Unsupervised Learning}] A category of machine learning where the model learns patterns from data without labeled responses.
\item[{Workflow}] The sequence of steps in a machine learning process including data collection, cleaning, modeling, and deployment.
\end{description}


\subsection{Section 2: Exploratory Data Analysis and Feature Engineering}
\label{sec:org2e7cdec}
\begin{description}
\item[{Analysis}] The phase in ML workflow where insights are extracted from data, including EDA and feature engineering.
\item[{Boxplot}] A graphical representation of data distribution highlighting the median, quartiles, and outliers.
\item[{Data Leakage}] Occurs when information from outside the training dataset is used to create the model, leading to overly optimistic results.
\item[{EDA (Exploratory Data Analysis)}] A process of analyzing datasets to summarize their main characteristics, often using visual methods.
\item[{Feature Extraction}] Creating new features from raw data, often by dimensionality reduction techniques like PCA or t-SNE.
\item[{Feature Selection}] Choosing a subset of relevant features for use in model construction, e.g., using RFE.
\item[{Histogram}] A plot showing the frequency distribution of a dataset, useful for identifying skewness and modality.
\item[{Joint Distribution}] A probability distribution over multiple random variables describing their simultaneous behavior.
\item[{KL Divergence}] A non-symmetric measure of how one probability distribution diverges from a second, expected distribution.
\item[{Marginal Distribution}] The distribution of a single variable within a joint distribution, integrating out others.
\item[{Mutual Information}] A measure of the mutual dependence between two variables, capturing both linear and nonlinear associations.
\item[{Normalization/Scaling}] Techniques such as MinMax or StandardScaler that adjust features to a common scale.
\item[{Pair Plot}] A matrix of scatter plots to visualize pairwise relationships between variables.
\item[{Q-Q Plot}] A quantile-quantile plot used to assess if a dataset follows a particular distribution.
\item[{Recursive Feature Elimination (RFE)}] An iterative method to select features by recursively removing the least important features.
\item[{Skewness}] A measure of asymmetry in the distribution of data.
\item[{StandardScaler}] A normalization technique that rescales features to have zero mean and unit variance.
\item[{t-SNE}] A nonlinear dimensionality reduction technique that visualizes high-dimensional data by preserving local structure.
\item[{UMAP}] A dimensionality reduction technique that preserves both local and global structure in data.
\end{description}

\subsection{Section 3: Model Training and Evaluation}
\label{sec:orgbf9472e}
\begin{description}
\item[{Bias-Variance Tradeoff}] The tension between model complexity and prediction error: simpler models have high bias and low variance, while complex models have low bias and high variance.
\item[{Cross-Validation}] A technique for estimating the generalization error of a model by training and evaluating it on different data subsets.
\item[{Empirical Risk}] The average loss computed on the training data; also called training error.
\item[{Evaluation Metrics}] Quantitative measures used to assess the performance of a machine learning model, e.g., accuracy, precision, recall, MSE.
\item[{Expected Risk}] The true generalization error computed over the full data distribution.
\item[{F1 Score}] The harmonic mean of precision and recall, used as a balanced performance metric in classification tasks.
\item[{Generalization Gap}] The difference between expected risk and empirical risk; large gaps indicate overfitting.
\item[{Loss Function}] A function that quantifies the error between predicted and true values; minimized during training.
\item[{Mean Squared Error (MSE)}] A regression metric that averages the squared differences between predicted and true values.
\item[{Model Selection}] The process of choosing the best-performing model based on validation performance, not test data.
\item[{Model Training}] The process of fitting a model's parameters to minimize a specified loss function on training data.
\item[{Overfitting}] A situation where a model performs well on training data but poorly on unseen data due to high variance.
\item[{Precision}] The proportion of true positives among all predicted positives in a classification task.
\item[{Recall}] The proportion of true positives detected among all actual positives in a classification task.
\item[{RÂ² Score}] A metric for regression that indicates the proportion of variance explained by the model.
\item[{Training Error}] The average loss of a model on training data; also referred to as empirical risk.
\item[{Validation Set}] A subset of the dataset used to tune model hyperparameters and evaluate performance during model selection.
\item[{Workflow}] The structured pipeline of steps in machine learning, including data preparation, modeling, and evaluation.
\end{description}
\end{document}
