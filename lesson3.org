#+TITLE: Introduction to Machine Learning: Modeling, Training and Evaluation
#+AUTHOR: Evan Misshula
#+DATE: \today
#+LANGUAGE: en

#+LATEX_HEADER: \usepackage[style=apa, backend=biber]{biblatex}
#+LATEX_HEADER: \DeclareLanguageMapping{american}{american-apa}
#+LATEX_HEADER: \addbibresource{./refs/refs.bib}
#+LATEX_HEADER: \AtEveryBibitem{\clearfield{note}}
#+LATEX_HEADER: \usepackage{endnotes}
#+LATEX_HEADER: \let\footnote=\endnote
#+LATEX_HEADER: \usepackage{./jtc}
#+STARTUP: beamer
#+OPTIONS: H:2 toc:nil num:t
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [aspectratio=169]
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
g
#+name: initialize_lang
#+source: configuration
#+begin_src emacs-lisp :results output :exports none
    (require 'ob-mermaid)
    (setq ob-mermaid-cli-path "/home/evan/.nvm/versions/node/v20.1.0/bin/mmdc")
    ;; Doesn't work
	       ;; first it is necessary to ensure that Org-mode loads support for the
		;; languages used by code blocks in this article
		(org-babel-do-load-languages
		 'org-babel-load-languages
		 '(
		   (ditaa      . t)     
		   (dot        . t)
		   (emacs-lisp . t)
		   (haskell    . t)
		   (org        . t)
		   (perl       . t)
		   (python     . t)
		   (R          . t)
		   (ruby       . t)
		   (plantuml   . t)
		   (mermaid    . t)
		   (sqlite     . t)))
		;; then we'll remove the need to confirm evaluation of each code
		;; block, NOTE: if you are concerned about execution of malicious code
		;; through code blocks, then comment out the following line
	    (add-to-list 'org-src-lang-modes '("plantuml" . plantuml))
	    (setq org-confirm-babel-evaluate nil)
	      (setq org-ditaa-jar-path "/usr/bin/ditaa")
	      (setq org-plantuml-jar-path "/usr/share/plantuml/plantuml.jar")
	      (add-to-list 'exec-path "/home/evan/.nvm/versions/node/v20.1.0/bin")
	;;      (setq org-mermaid-jar-path "/home/evan/.nvm/versions/node/v20.1.0/lib/node_modules/@mermaid-js/mermaid-cli/node_modules/mermaid
	;;    ")
      (setenv "PATH" (concat (getenv "PATH") ":/home/evan/.nvm/versions/node/v20.1.0/bin"))
      (add-to-list 'exec-path "/home/evan/.nvm/versions/node/v20.1.0/bin")

	     (setenv "PUPPETEER_EXECUTABLE_PATH" "/usr/bin/google-chrome-stable")
	     (setenv "PUPPETEER_DISABLE_SANDBOX" "1")
    (setq org-babel-mermaid-cli-path "/home/evan/.nvm/versions/node/v20.1.0/bin/mmdc")

(setq org-preview-latex-default-process 'dvipng)
(setq org-preview-latex-process-alist
      '((dvipng :programs ("latex" "dvipng")
                :description "dvi > png using dvipng"
                :message "You need to install latex and dvipng"
                :image-input-type "dvi"
                :image-output-type "png"
                :image-size-adjust (1.0 . 1.0)
                :latex-compiler ("latex -interaction nonstopmode -output-directory %o %f")
                :image-converter ("dvipng -D 300 -T tight -o %O %f"))))

(setq org-preview-latex-image-directory "ltximg/")

      ;; Add LaTeX block template and scaling
      (with-eval-after-load 'org
	(add-to-list 'org-structure-template-alist '("e" . "latex"))
	(plist-put org-format-latex-options :scale 3.0))


	     (setenv "PATH" (concat "/home/evan/.nvm/versions/node/v20.1.0/bin:" (getenv "PATH")))
	      ;; finally we'll customize the default behavior of Org-mode code blocks
		;; so that they can be used to display examples of Org-mode syntax
		(setf org-babel-default-header-args:org '((:exports . "code")))
		(setq org-babel-inline-result-wrap '%s)
		;; This gets rid of the wrapping around the results of evaluated org mode 
		;; in line code
		(setq reftex-default-bibliography '("/home/emisshula/proposal/mybib.bib"))
		(setq org-latex-prefer-user-labels t)
    ;;    (plist-put org-format-latex-options :scale 3.0)
	(global-set-key (kbd "C-c e") 'insEq)
#+end_src

#+RESULTS: configuration

* Workflow
** End to end process                                          :B_definition:
:PROPERTIES:
:BEAMER_env: definition
:END:
Recall *ML workflow* is a sequence of steps to build and deploy a model that
solves a problem using data.

** The pipeline                                                     :B_block::
:BEAMER_env: block
:END:

| Ingestion & Preprocessing | Analysis            | *Modeling*   | Deployment |
|---------------------------+---------------------+--------------+------------|
| Definition                | EDA                 | *Selection*  | Tuning     |
| Data Collection           | Feature Engineering | *Training*   | Deployment |
| Cleaning                  |                     | *Evaluation* | Monitoring |

** ML Workflow Graph                                                :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
#+CAPTION: ML workflow steps rendered as a flowchart
#+ATTR_LATEX: :width=0.8\linewidth
[[file:workflow.png]]


#+begin_src mermaid :file workflow.png  :exports results
  graph LR
    A[Ingestion] --> B[Analysis]
    B --> C[Modeling]
    C --> D[Deployment]
#+end_src

#+RESULTS:
[[file:workflow.png]]

* Training
** What is Model Training?                                          :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Model training is the process of estimating parameters $\theta$ of a model $f_\theta(x)$ using data $\{(x_i, y_i)\}_{i=1}^n$.
- Typically achieved by minimizing a loss function:
  \begin{equation}
  \hat{\theta} = \arg\min_\theta \frac{1}{n} \sum_{i=1}^n \mathcal{L}(f_\theta(x_i), y_i)
  \end{equation}
- Common loss functions:
  - **Squared error loss** (regression): $\mathcal{L}(\hat{y}, y) = (\hat{y} - y)^2$
  - **Cross-entropy loss** (classification): 
\begin{equation}
    \mathcal{L}(\hat{y}, y) = -\sum_{c} \1_{\{y = c\}} \log \hat{p}_c
\1_{\{x = 1\}}
\end{equation}



** Training vs Generalization                                       :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- **Empirical risk** (training error):
  \begin{equation}
  \hat{R}(\theta) = \frac{1}{n} \sum_{i=1}^n \mathcal{L}(f_\theta(x_i), y_i)
  \end{equation}
- **Expected risk** (true/generalization error):
  \begin{equation}
  R(\theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}} \left[ \mathcal{L}(f_\theta(x), y) \right]
  \end{equation}
- Generalization gap: $R(\theta) - \hat{R}(\theta)$
- Overfitting: small $\hat{R}$, large $R$

* Evaluation  
** Evaluation Metrics                                               :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- *Regression*:
  - Mean Squared Error (MSE): 
    \[
    \text{MSE} = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2
    \]
  - $R^2$ score:
    \[
    R^2 = 1 - \frac{\sum_i (\hat{y}_i - y_i)^2}{\sum_i (y_i - \bar{y})^2}
    \]

- *Classification*:
  - Accuracy: \(\text{Accuracy} = \frac{1}{n} \sum_{i=1}^n \mathds{1}_{\{\hat{y}_i = y_i\}}\)
  - Precision: \(\frac{\text{TP}}{\text{TP} + \text{FP}}\)
  - Recall: \(\frac{\text{TP}}{\text{TP} + \text{FN}}\)
  - F1 score: harmonic mean of precision and recall
    \[
    F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
    \]

** Cross-Validation                                                 :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Cross-validation estimates generalization error by partitioning data.
- *k-fold CV*:
  - Split data into $k$ disjoint subsets.
  - For each $i = 1, \ldots, k$:
    - Train on $k-1$ folds
    - Evaluate on fold $i$
  - Average the evaluation metrics.

** Bias-Variance Tradeoff                                           :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Expected prediction error at point $x$:
  \[
  \mathbb{E}[(f(x) - y)^2] = \underbrace{[\mathbb{E}(f(x)) - y]^2}_{\text{Bias}^2} + \underbrace{\mathbb{E}[(f(x) - \mathbb{E}(f(x)))^2]}_{\text{Variance}} + \underbrace{\sigma^2}_{\text{Irreducible error}}
  \]
- Simple models: low variance, high bias
- Complex models: low bias, high variance

** Model Selection                                                  :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Choose the best model using a *validation set* or *cross-validation*.
- Avoid tuning hyperparameters using the test set.
- Balance:
  - Training error
  - Generalization performance
  - Computational cost

** Summary Training and Evaluation                                  :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
- Training minimizes empirical loss.
- Evaluation uses test or validation data.
- Use metrics appropriate for the task.
- Cross-validation provides robust error estimates.
- The bias-variance tradeoff is fundamental in choosing models.
